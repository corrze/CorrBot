{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b58781",
   "metadata": {},
   "source": [
    "Download these packages and any other missing packages\n",
    "Use Python 3.10.11 Kernal if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52369bdb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets peft accelerate bitsandbytes xformers huggingface-cli NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc41f67",
   "metadata": {},
   "source": [
    "log into hugging face account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4d237",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hf authen login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189990f",
   "metadata": {},
   "source": [
    "Importing packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae97a41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorForSeq2Seq, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88eae3f",
   "metadata": {},
   "source": [
    "Setting your configs and loading the dataset.\n",
    "Insert the path of your .jsonl file and the directory you want your model to be\n",
    "\n",
    "Example:\n",
    "DATA_PATH = r\"K:\\ml_datasets\\messages\\prompt_response.jsonl\"\n",
    "OUTPUT_DIR = r\"K:\\ml_datasets\\messages\\llama3_8b_corrbot_qlora_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4de2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\" # Can change if you're using a different model\n",
    "DATA_PATH = r\"\" # Remove the \"r\" of you are on mac or linux. I am on windows.\n",
    "OUTPUT_DIR = r\"\" # insert path and name of model here\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "MAX_LENGTH = 490 # Change this number to your largest conversation. Use Token_counter.py to check the number of token of your longest convo\n",
    "EPOCHS = 3\n",
    "\n",
    "# === LOAD TOKENIZER AND MODEL ===\n",
    "raw_dataset = load_dataset(\"json\", data_files=DATA_PATH)[\"train\"]\n",
    "dataset = raw_dataset.train_test_split(test_size=0.1, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1ffb0",
   "metadata": {},
   "source": [
    "Setting up Tokenizers and Tokenizations Function\n",
    "\n",
    "I used DataCollatorForSeq25Seq because it is good for Input-Output Sequence Pairs. It pads tokens with the label -100 during traiing as the loss function will usually ignore those tokens. Our labels would be very inaccurate if we have -100 repeating in our labels so we filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da378d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Tokenization function\n",
    "def format(example):\n",
    "    tokens = tokenizer(example[\"raw_prompt\"], truncation=True)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "# Split and tokenize datasets\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "tokenized_train = train_dataset.map(format, remove_columns=['prompt', 'raw_prompt', 'raw_response'])\n",
    "tokenized_val = val_dataset.map(format, remove_columns=['prompt', 'raw_prompt', 'raw_response'])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    pad_to_multiple_of=8,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46aa5d7",
   "metadata": {},
   "source": [
    "Loading the Model and applying LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1bd33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === LOAD MODEL ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# === LoRA CONFIGURATION ===\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout = 0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd7234",
   "metadata": {},
   "source": [
    "Defining training argumments and callbacks to check your gpu status while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25331267",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training setup\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    optim =\"paged_adamw_8bit\",\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"steps\", # For smaller models you can change this to \"epoches\" or even \"no\" if its too small\n",
    "    save_steps=1000, # If \"no\" can delete this\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=100,\n",
    "    save_safetensors=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "class MemoryClearCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 50 == 0:\n",
    "            print(torch.cuda.memory_summary())\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea1c35",
   "metadata": {},
   "source": [
    "Setting up Trainer and Fine-tuning the model\n",
    "\n",
    "If you have everything installed it should work, if not try to install everything neccessary to run the model. \n",
    "\n",
    "Make sure \"resume_from_checkpoint=False\" unless your training crashed\n",
    "and you want to resume from where ever your checkpoints are. Then you can set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50891a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === TRAINER SETUP ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[MemoryClearCallback()]\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
